{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "egiKc7xAnrFU",
        "DxOw7S6skeln",
        "tB871sZrXvBb",
        "v3VRT_iQZCAc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "egiKc7xAnrFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-genai\n",
        "!pip install -q -U pydantic\n",
        "!pip install -q -U langchain langchain-google-genai\n",
        "!pip install -q -U langgraph\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2l_UKlA5aNR",
        "outputId": "e789bec0-55bf-4126-87a9-c0124765f1ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from enum import Enum, auto\n",
        "from typing import Optional, Dict\n",
        "\n",
        "import os\n",
        "from google import genai\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Literal, List\n",
        "from typing import Union, List, Any\n",
        "from google.genai import types\n",
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "# Imports Modernos (LangGraph + LangChain Core)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage\n",
        "from langgraph.prebuilt import create_react_agent\n"
      ],
      "metadata": {
        "id": "lAPkG8fVG46e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEYL')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n"
      ],
      "metadata": {
        "id": "X4YnseuH3lMp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ›ï¸ Fase 1: La Era Determinista (Enfoque Tradicional)\n",
        "\n",
        "En esta primera etapa, viajamos al pasado para ver cÃ³mo se programaban los chatbots antes de la IA Generativa. Este enfoque se basa en la **programaciÃ³n imperativa explÃ­cita**.\n",
        "\n",
        "**Conceptos Clave:**\n",
        "* **MÃ¡quina de Estados (State Machine):** El cÃ³digo controla rÃ­gidamente en quÃ© paso de la conversaciÃ³n estamos (Esperando -> Pidiendo Nombre -> Confirmando).\n",
        "* **Regex (Expresiones Regulares):** Buscamos patrones exactos de texto.\n",
        "* **La Fragilidad:** Si el usuario dice \"apartame un sitio\" en lugar de \"reserva una mesa\", el bot falla porque esa palabra no estÃ¡ en su lista permitida.\n",
        "\n",
        "**Objetivo:** Demostrar la carga cognitiva del programador, quien debÃ­a prever cada posible sinÃ³nimo y ruta de conversaciÃ³n."
      ],
      "metadata": {
        "id": "DxOw7S6skeln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definimos los estados posibles de la conversaciÃ³n\n",
        "# En el pasado, el programador debÃ­a \"recordar\" manualmente en quÃ© paso estaba el usuario.\n",
        "class BotState(Enum):\n",
        "    ESPERANDO = auto()\n",
        "    PIDIENDO_NOMBRE = auto()\n",
        "    PIDIENDO_PERSONAS = auto()\n",
        "    CONFIRMACION = auto()\n",
        "\n",
        "class RestauranteBotLegacy:\n",
        "    \"\"\"\n",
        "    Bot tradicional basado en reglas y expresiones regulares.\n",
        "    No tiene 'comprensiÃ³n', solo busca patrones de texto predefinidos.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state: BotState = BotState.ESPERANDO\n",
        "        self.reserva_data: Dict[str, str] = {}\n",
        "\n",
        "        # Patrones \"Hardcoded\" (La fragilidad del sistema)\n",
        "        # Si el usuario usa una palabra que no estÃ¡ aquÃ­ (ej: \"apartar lugar\"), el bot falla.\n",
        "        self.patrones_reserva = r\"\\b(reservar|mesa|lugar|reserva)\\b\"\n",
        "        self.patrones_saludo = r\"\\b(hola|buenos dias|buenas)\\b\"\n",
        "\n",
        "    def procesar_input(self, texto_usuario: str) -> str:\n",
        "        \"\"\"Procesa el texto basÃ¡ndose en el estado actual y regex.\"\"\"\n",
        "        texto_normalizado = texto_usuario.lower().strip()\n",
        "\n",
        "        # MAQUINA DE ESTADOS: LÃ³gica de flujo explÃ­cita\n",
        "\n",
        "        # 1. Estado: Esperando intenciÃ³n inicial\n",
        "        if self.state == BotState.ESPERANDO:\n",
        "            if re.search(self.patrones_reserva, texto_normalizado):\n",
        "                self.state = BotState.PIDIENDO_NOMBRE\n",
        "                return \"Â¡Claro! Para gestionar tu reserva, Â¿a nombre de quiÃ©n la anoto?\"\n",
        "\n",
        "            elif re.search(self.patrones_saludo, texto_normalizado):\n",
        "                return \"Â¡Hola! Bienvenido a 'Python Bistro'. Escribe 'quiero reservar' para comenzar.\"\n",
        "\n",
        "            else:\n",
        "                return \"Disculpa, no entendÃ­. Solo puedo gestionar 'reservas' por ahora.\"\n",
        "\n",
        "        # 2. Estado: Recopilando Nombre\n",
        "        elif self.state == BotState.PIDIENDO_NOMBRE:\n",
        "            self.reserva_data[\"nombre\"] = texto_usuario.title()\n",
        "            self.state = BotState.PIDIENDO_PERSONAS\n",
        "            return f\"Gracias {self.reserva_data['nombre']}. Â¿Para cuÃ¡ntas personas es la mesa?\"\n",
        "\n",
        "        # 3. Estado: Recopilando Cantidad\n",
        "        elif self.state == BotState.PIDIENDO_PERSONAS:\n",
        "            # Intentamos extraer un nÃºmero del texto\n",
        "            numeros = re.findall(r\"\\d+\", texto_normalizado)\n",
        "\n",
        "            if numeros:\n",
        "                self.reserva_data[\"personas\"] = numeros[0]\n",
        "                self.state = BotState.CONFIRMACION\n",
        "                return f\"Perfecto. Mesa para {self.reserva_data['personas']}. Â¿Confirmas la reserva? (si/no)\"\n",
        "            else:\n",
        "                # Manejo de error rÃ­gido\n",
        "                return \"Por favor, escribe el nÃºmero de personas (ej: 2, 4).\"\n",
        "\n",
        "        # 4. Estado: ConfirmaciÃ³n final\n",
        "        elif self.state == BotState.CONFIRMACION:\n",
        "            if \"si\" in texto_normalizado:\n",
        "                # Reset del estado para la prÃ³xima vez\n",
        "                resumen = f\"Â¡Listo! Reserva confirmada para {self.reserva_data['nombre']} ({self.reserva_data['personas']} personas).\"\n",
        "                self._reset_bot()\n",
        "                return resumen\n",
        "            elif \"no\" in texto_normalizado:\n",
        "                self._reset_bot()\n",
        "                return \"Entendido, reserva cancelada. Â¿En quÃ© mÃ¡s puedo ayudarte?\"\n",
        "            else:\n",
        "                return \"Por favor responde 'si' o 'no'.\"\n",
        "\n",
        "        return \"Error de estado.\"\n",
        "\n",
        "    def _reset_bot(self):\n",
        "        \"\"\"Reinicia la memoria del bot.\"\"\"\n",
        "        self.state = BotState.ESPERANDO\n",
        "        self.reserva_data = {}\n",
        "\n",
        "# --- Bloque de Prueba (SimulaciÃ³n) ---\n",
        "if __name__ == \"__main__\":\n",
        "    bot = RestauranteBotLegacy()\n",
        "\n",
        "    print(\"ğŸ¤– Bot Tradicional Iniciado (Escribe 'salir' para terminar)\\n\")\n",
        "\n",
        "    # Simulamos una conversaciÃ³n simple\n",
        "    inputs_prueba = [\n",
        "        \"Hola buenas\",\n",
        "        \"Quiero reservar una mesa\",\n",
        "        \"Juan Perez\",\n",
        "        \"Seremos 4 personas\",\n",
        "        \"Si, confirmo\"\n",
        "    ]\n",
        "\n",
        "    for mensaje in inputs_prueba:\n",
        "        print(f\"Usuario: {mensaje}\")\n",
        "        respuesta = bot.procesar_input(mensaje)\n",
        "        print(f\"Bot:     {respuesta}\\n\")"
      ],
      "metadata": {
        "id": "azan0NiNUmhY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9ebfd5b-b07f-4bf0-913f-ee373d427c49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Bot Tradicional Iniciado (Escribe 'salir' para terminar)\n",
            "\n",
            "Usuario: Hola buenas\n",
            "Bot:     Â¡Hola! Bienvenido a 'Python Bistro'. Escribe 'quiero reservar' para comenzar.\n",
            "\n",
            "Usuario: Quiero reservar una mesa\n",
            "Bot:     Â¡Claro! Para gestionar tu reserva, Â¿a nombre de quiÃ©n la anoto?\n",
            "\n",
            "Usuario: Juan Perez\n",
            "Bot:     Gracias Juan Perez. Â¿Para cuÃ¡ntas personas es la mesa?\n",
            "\n",
            "Usuario: Seremos 4 personas\n",
            "Bot:     Perfecto. Mesa para 4. Â¿Confirmas la reserva? (si/no)\n",
            "\n",
            "Usuario: Si, confirmo\n",
            "Bot:     Â¡Listo! Reserva confirmada para Juan Perez (4 personas).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  Fase 2: La Era SemÃ¡ntica (IA Generativa + Estructura)\n",
        "\n",
        "Damos el salto a la IA con **Gemini 2.5 Flash**. AquÃ­ abandonamos las reglas rÃ­gidas (`if/else`) y confiamos en la capacidad del modelo para \"entender\" la intenciÃ³n.\n",
        "\n",
        "**Conceptos Clave:**\n",
        "* **ComprensiÃ³n Natural:** El modelo entiende contextos complejos, errores ortogrÃ¡ficos y jergas.\n",
        "* **Salida Estructurada (JSON):** Usamos el SDK moderno `google-genai` y **Pydantic** para obligar al modelo a devolver datos limpios y tipados, no solo texto conversacional.\n",
        "* **La LimitaciÃ³n:** El \"Cerebro en una Caja\". El modelo puede *decir* que reservÃ³ la mesa, pero es una alucinaciÃ³n. No tiene conexiÃ³n real con la base de datos ni capacidad de acciÃ³n."
      ],
      "metadata": {
        "id": "tB871sZrXvBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Definimos el Esquema (Modelos Pydantic)\n",
        "class DetallesReserva(BaseModel):\n",
        "    nombre_cliente: str | None = Field(description=\"Nombre del cliente\")\n",
        "    cantidad_personas: int | None = Field(description=\"NÃºmero de comensales\")\n",
        "    hora: str | None = Field(description=\"Hora de la reserva\")\n",
        "\n",
        "class RespuestaBot(BaseModel):\n",
        "    mensaje_para_usuario: str\n",
        "    estado_reserva: DetallesReserva\n",
        "    falta_informacion: bool\n",
        "\n",
        "# 2. Cliente Moderno\n",
        "#client = genai.Client() -> ejecutado en el Setup\n",
        "\n",
        "def procesar_input_moderno(texto_usuario: str):\n",
        "    try:\n",
        "        # MODELO: Usamos 'gemini-2.5-flash'\n",
        "        response = client.models.generate_content(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=texto_usuario,\n",
        "            config=types.GenerateContentConfig(\n",
        "                response_mime_type=\"application/json\",\n",
        "                response_schema=RespuestaBot,\n",
        "                system_instruction=\"Eres un asistente de restaurante. Extrae datos y responde con formato JSON estricto.\",\n",
        "                temperature=0\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Parseo seguro\n",
        "        import json\n",
        "        json_data = json.loads(response.text)\n",
        "        return RespuestaBot(**json_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error del Modelo: {e}\"\n",
        "\n",
        "# Prueba\n",
        "print(\"Enviando peticiÃ³n a Gemini 2.5 Flash...\")\n",
        "print(procesar_input_moderno(\"Mesa para 4 personas a las 8pm, soy Luis.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lbh0fn2QYyqV",
        "outputId": "df87685e-6390-4f52-c5ac-ce82ccc2b5b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enviando peticiÃ³n a Gemini 2.5 Flash...\n",
            "mensaje_para_usuario='Perfecto Luis, su mesa para 4 personas a las 8pm estÃ¡ confirmada.' estado_reserva=DetallesReserva(nombre_cliente='Luis', cantidad_personas=4, hora='8pm') falta_informacion=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ¤– Fase 3: La Era AgÃ©ntica (LangChain + LangGraph + Gemini 2.5)\n",
        "\n",
        "Esta es la implementaciÃ³n mÃ¡s avanzada (Estado del Arte 2025). Creamos un **Agente AutÃ³nomo** capaz de Razonar y Actuar (PatrÃ³n ReAct).\n",
        "\n",
        "**Mejoras TÃ©cnicas Implementadas:**\n",
        "1.  **LangGraph:** Gestionamos la conversaciÃ³n como un grafo de estados, permitiendo bucles de razonamiento.\n",
        "2.  **InyecciÃ³n de Estado:** Para mÃ¡xima compatibilidad, inyectamos las reglas del sistema (`SystemMessage`) directamente en la memoria del agente, haciÃ©ndolo robusto frente a cambios de versiones de librerÃ­as.\n",
        "3.  **Parseo Multimodal Robusto:** Implementamos una funciÃ³n de limpieza que normaliza la respuesta de Gemini 2.5, manejando tanto texto plano como respuestas complejas con metadatos.\n",
        "4.  **Seguridad:** GestiÃ³n segura de API Keys mediante `userdata`.\n",
        "\n",
        "**Objetivo:** Observa cÃ³mo el Agente *verifica* disponibilidad antes de *ejecutar* la reserva en la base de datos real."
      ],
      "metadata": {
        "id": "v3VRT_iQZCAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------\n",
        "# 1. SEGURIDAD Y CONFIGURACIÃ“N\n",
        "# ---------------------------------------------------------\n",
        "try:\n",
        "    # Recuperamos la clave de forma segura\n",
        "    MY_API_KEY = userdata.get('GOOGLE_API_KEYL')\n",
        "except Exception as e:\n",
        "    raise ValueError(\"âš ï¸ Error: No se pudo acceder a 'GOOGLE_API_KEY' en los secretos de Colab. AsegÃºrate de haberla aÃ±adido en el icono de la llave ğŸ”‘ a la izquierda.\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. BASE DE DATOS Y HERRAMIENTAS\n",
        "# ---------------------------------------------------------\n",
        "DB_RESERVAS = {\n",
        "    \"reservas\": [],\n",
        "    \"mesas_disponibles\": [\"19:00\", \"20:00\", \"21:00\"]\n",
        "}\n",
        "\n",
        "MENU_CONTEXTO = \"\"\"\n",
        "El menÃº del restaurante 'Python Bistro':\n",
        "- Especialidad: Risotto de setas con trufa negra (25â‚¬).\n",
        "- Entrada: Ensalada CÃ©sar (12â‚¬), Carpaccio de res (15â‚¬).\n",
        "- Postres: TiramisÃº casero (8â‚¬), VolcÃ¡n de chocolate (9â‚¬).\n",
        "- Nota: Tenemos opciones sin gluten y veganas bajo peticiÃ³n.\n",
        "\"\"\"\n",
        "\n",
        "@tool\n",
        "def consultar_menu(pregunta: str) -> str:\n",
        "    \"\"\"Ãštil para preguntas sobre el menÃº, platos y precios.\"\"\"\n",
        "    return f\"InformaciÃ³n del menÃº: {MENU_CONTEXTO}\"\n",
        "\n",
        "@tool\n",
        "def verificar_disponibilidad(hora: str) -> str:\n",
        "    \"\"\"Verifica mesas disponibles (formato HH:MM).\"\"\"\n",
        "    if hora in DB_RESERVAS[\"mesas_disponibles\"]:\n",
        "        return \"Disponible\"\n",
        "    return \"No disponible. Horas libres: \" + \", \".join(DB_RESERVAS[\"mesas_disponibles\"])\n",
        "\n",
        "@tool\n",
        "def guardar_reserva(nombre: str, hora: str, personas: int) -> str:\n",
        "    \"\"\"Guarda una reserva. SOLO usar si 'verificar_disponibilidad' retornÃ³ 'Disponible'.\"\"\"\n",
        "    if hora in DB_RESERVAS[\"mesas_disponibles\"]:\n",
        "        DB_RESERVAS[\"mesas_disponibles\"].remove(hora)\n",
        "        DB_RESERVAS[\"reservas\"].append({\"nombre\": nombre, \"hora\": hora, \"personas\": personas})\n",
        "        return f\"Ã‰XITO: Reserva confirmada para {nombre} a las {hora}.\"\n",
        "    else:\n",
        "        return \"ERROR: La mesa ya no estÃ¡ disponible.\"\n",
        "\n",
        "tools = [consultar_menu, verificar_disponibilidad, guardar_reserva]\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. CONSTRUCCIÃ“N DEL AGENTE\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Inicializamos el modelo pasando la clave EXPLÃCITAMENTE\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    google_api_key=MY_API_KEY  # <--- AQUÃ ESTÃ LA CLAVE\n",
        ")\n",
        "\n",
        "# Definimos el Prompt del Sistema\n",
        "SYSTEM_INSTRUCTIONS= \"\"\"\n",
        "Eres el Agente Inteligente del restaurante 'Python Bistro'.\n",
        "Tienes acceso a herramientas para consultar menÃº, ver disponibilidad y reservar.\n",
        "\n",
        "REGLAS:\n",
        "- Antes de reservar, SIEMPRE verifica la disponibilidad.\n",
        "- Si no hay mesa, ofrece otra hora.\n",
        "- SÃ© amable pero eficiente.\n",
        "\"\"\"\n",
        "\n",
        "# Usamos 'prompt' que es el estÃ¡ndar actual en LangGraph prebuilt\n",
        "# para inyectar el System Prompt.\n",
        "agent_graph = create_react_agent(\n",
        "    model=llm,\n",
        "    tools=tools,\n",
        "    prompt=SYSTEM_INSTRUCTIONS\n",
        ")\n",
        "\n",
        "def obtener_texto_limpio(contenido: Union[str, List[Any], Any]) -> str:\n",
        "    \"\"\"\n",
        "    Normaliza la respuesta de Gemini a un string plano.\n",
        "    Maneja tipos str, list (multimodal) y objetos desconocidos.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Caso Base: Si ya es texto, retornamos directo.\n",
        "    if isinstance(contenido, str):\n",
        "        return contenido\n",
        "\n",
        "    # 2. Caso Multimodal: Si es lista, procesamos eficientemente.\n",
        "    if isinstance(contenido, list):\n",
        "        texto_partes = []\n",
        "\n",
        "        for parte in contenido:\n",
        "            if isinstance(parte, dict):\n",
        "                # .get('key', '') es mÃ¡s seguro que ['key'] porque no falla si la clave no existe\n",
        "                texto_partes.append(parte.get('text', ''))\n",
        "\n",
        "            elif hasattr(parte, 'text'):\n",
        "                texto_partes.append(str(parte.text))\n",
        "\n",
        "            else:\n",
        "                # Si hay un elemento raro en la lista, lo convertimos a string para no perderlo\n",
        "                texto_partes.append(str(parte))\n",
        "\n",
        "        # 'join' es la forma pythonica y optimizada de unir texto\n",
        "        return \"\".join(texto_partes)\n",
        "\n",
        "    # 3. Fallback (Red de seguridad):\n",
        "    # Si llega un entero, None, o un objeto desconocido, lo convertimos a string.\n",
        "    return str(contenido)\n",
        "\n",
        "def imprimir_respuesta(respuesta_graph: dict):\n",
        "    mensajes = respuesta_graph[\"messages\"]\n",
        "    ultimo_mensaje = mensajes[-1]\n",
        "\n",
        "    # Usamos la funciÃ³n de limpieza\n",
        "    texto_final = obtener_texto_limpio(ultimo_mensaje.content)\n",
        "    print(f\"ğŸ¤– Agente: {texto_final}\\n\")\n",
        "    #print(f\"ğŸ¤– Agente: {ultimo_mensaje.content}\\n\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. EJECUCIÃ“N\n",
        "# ---------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ğŸš€ AGENTE INICIADO (Gemini 2.5 + LangGraph + LangChain)\\n\")\n",
        "\n",
        "    # ESTRATEGIA DE INYECCIÃ“N DE ESTADO:\n",
        "    # Iniciamos el historial poniendo el SystemMessage primero.\n",
        "    # El modelo leerÃ¡ esto y sabrÃ¡ cÃ³mo comportarse, sin importar la versiÃ³n de la librerÃ­a.\n",
        "    chat_history: List[BaseMessage] = []\n",
        "\n",
        "    # --- CASO 1 ---\n",
        "    print(\"--- CASO 1: Consulta de MenÃº ---\")\n",
        "    input_1 = \"Hola, Â¿tienen opciones veganas?\"\n",
        "    print(f\"ğŸ‘¤ Usuario: {input_1}\")\n",
        "\n",
        "    chat_history.append(HumanMessage(content=input_1))\n",
        "\n",
        "    # Invocamos al agente pasando todo el historial\n",
        "    response_1 = agent_graph.invoke({\"messages\": chat_history})\n",
        "\n",
        "    chat_history = response_1[\"messages\"]\n",
        "    imprimir_respuesta(response_1)\n",
        "\n",
        "    # --- CASO 2 ---\n",
        "    print(\"--- CASO 2: Intento Reserva (22:00 Lleno) ---\")\n",
        "    input_2 = \"Quiero reservar para 4 personas a las 22:00.\"\n",
        "    print(f\"ğŸ‘¤ Usuario: {input_2}\")\n",
        "\n",
        "    chat_history.append(HumanMessage(content=input_2))\n",
        "    response_2 = agent_graph.invoke({\"messages\": chat_history})\n",
        "\n",
        "    chat_history = response_2[\"messages\"]\n",
        "    imprimir_respuesta(response_2)\n",
        "\n",
        "    # --- CASO 3 ---\n",
        "    print(\"--- CASO 3: CorrecciÃ³n (21:00 Disponible) ---\")\n",
        "    input_3 = \"Entendido, entonces a las 21:00 a nombre de Ana.\"\n",
        "    print(f\"ğŸ‘¤ Usuario: {input_3}\")\n",
        "\n",
        "    chat_history.append(HumanMessage(content=input_3))\n",
        "    response_3 = agent_graph.invoke({\"messages\": chat_history})\n",
        "\n",
        "    chat_history = response_3[\"messages\"]\n",
        "    imprimir_respuesta(response_3)\n",
        "\n",
        "    # --- CASO 4 ---\n",
        "    print(\"--- CASO 4: EstimaciÃ³n de Gastos ---\")\n",
        "    input_4 = \"Â¿CuÃ¡nto serÃ­a el gasto aproximado por persona, pidiendo los tres platos?\"\n",
        "    print(f\"ğŸ‘¤ Usuario: {input_4}\")\n",
        "\n",
        "    chat_history.append(HumanMessage(content=input_4))\n",
        "    response_4 = agent_graph.invoke({\"messages\": chat_history})\n",
        "\n",
        "    chat_history = response_4[\"messages\"]\n",
        "    imprimir_respuesta(response_4)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(\"ğŸ” ESTADO FINAL DE LA BASE DE DATOS:\")\n",
        "    print(f\"Reservas guardadas: {DB_RESERVAS['reservas']}\")\n",
        "    print(f\"Disponibilidad restante: {DB_RESERVAS['mesas_disponibles']}\")"
      ],
      "metadata": {
        "id": "vSiB3FcUX1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a809f256-b4e3-462a-b7ab-38c9b5f7a359"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1542123833.py:75: LangGraphDeprecatedSinceV10: create_react_agent has been moved to `langchain.agents`. Please update your import to `from langchain.agents import create_agent`. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
            "  agent_graph = create_react_agent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ AGENTE INICIADO (Gemini 2.5 + LangGraph + LangChain)\n",
            "\n",
            "--- CASO 1: Consulta de MenÃº ---\n",
            "ğŸ‘¤ Usuario: Hola, Â¿tienen opciones veganas?\n",
            "ğŸ¤– Agente: SÃ­, tenemos opciones veganas bajo peticiÃ³n.\n",
            "\n",
            "--- CASO 2: Intento Reserva (22:00 Lleno) ---\n",
            "ğŸ‘¤ Usuario: Quiero reservar para 4 personas a las 22:00.\n",
            "ğŸ¤– Agente: Lo siento, a las 22:00 no tenemos disponibilidad. Las horas libres son: 19:00, 20:00, 21:00. Â¿Te gustarÃ­a reservar en alguna de esas horas?\n",
            "\n",
            "--- CASO 3: CorrecciÃ³n (21:00 Disponible) ---\n",
            "ğŸ‘¤ Usuario: Entendido, entonces a las 21:00 a nombre de Ana.\n",
            "ğŸ¤– Agente: Â¡Perfecto! Tu reserva para 4 personas a las 21:00 a nombre de Ana ha sido confirmada. Â¡Esperamos verte pronto!\n",
            "\n",
            "--- CASO 4: EstimaciÃ³n de Gastos ---\n",
            "ğŸ‘¤ Usuario: Â¿CuÃ¡nto serÃ­a el gasto aproximado por persona, pidiendo los tres platos?\n",
            "ğŸ¤– Agente: El gasto aproximado por persona, pidiendo una entrada, un plato principal y un postre, serÃ­a entre 45â‚¬ y 49â‚¬, dependiendo de los platos elegidos.\n",
            "\n",
            "--------------------------------------------------\n",
            "ğŸ” ESTADO FINAL DE LA BASE DE DATOS:\n",
            "Reservas guardadas: [{'nombre': 'Ana', 'hora': '21:00', 'personas': 4}]\n",
            "Disponibilidad restante: ['19:00', '20:00']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursos\n",
        "* [API de Gemini](https://ai.google.dev/api?hl=es-419)\n",
        "* [Pydantic](https://docs.pydantic.dev/latest/)\n",
        "* [LangChain](https://docs.langchain.com/)"
      ],
      "metadata": {
        "id": "U8M3lQyx6qfA"
      }
    }
  ]
}